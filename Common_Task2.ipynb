{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_TxVNNk6BTt"
   },
   "source": [
    "### Common Test 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-24T13:34:47.406643Z",
     "iopub.status.busy": "2025-03-24T13:34:47.406387Z",
     "iopub.status.idle": "2025-03-24T13:34:52.188861Z",
     "shell.execute_reply": "2025-03-24T13:34:52.187908Z",
     "shell.execute_reply.started": "2025-03-24T13:34:47.406620Z"
    },
    "id": "Qm0BCaBU3q1S",
    "outputId": "b62a63a5-3d3a-4553-b66f-889fb15219a3",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples processed: 15552\n",
      "Train set: 12441 examples\n",
      "Validation set: 1555 examples\n",
      "Test set: 1556 examples\n",
      "Data processing and splitting completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Regex pattern to tokenize expressions:\n",
    "# The pattern captures:\n",
    "#   - Words (letters and underscores)\n",
    "#   - Numbers (including those with decimal points)\n",
    "#   - Mathematical operators and punctuation (like +, -, *, /, ^, parentheses, braces, etc.)\n",
    "token_pattern = re.compile(r'([A-Za-z_]+|\\d+\\.\\d+|\\d+|[+\\-*/^(){}\\[\\]:,])')\n",
    "\n",
    "def normalize_indices(seq):\n",
    "    \"\"\"\n",
    "    Normalizes indices of the form _<number> in a given sequence.\n",
    "    Each unique index is replaced with a normalized index (_1, _2, ...)\n",
    "    in order of appearance.\n",
    "    \"\"\"\n",
    "    # Pattern to find indices like _123456\n",
    "    index_pattern = re.compile(r'_(\\d+)')\n",
    "    mapping = {}\n",
    "    normalized_seq = seq\n",
    "\n",
    "    # Find all indices in order of appearance\n",
    "    for match in index_pattern.finditer(seq):\n",
    "        full_token = match.group(0)  # e.g., '_239'\n",
    "        if full_token not in mapping:\n",
    "            # Assign the next available normalized index (starting at 1)\n",
    "            mapping[full_token] = f\"_{len(mapping) + 1}\"\n",
    "    # Replace the indices using the mapping.\n",
    "    for original, normalized in mapping.items():\n",
    "        normalized_seq = normalized_seq.replace(original, normalized)\n",
    "    return normalized_seq\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the given text using the token_pattern.\n",
    "    Returns a list of tokens.\n",
    "    \"\"\"\n",
    "    return token_pattern.findall(text)\n",
    "\n",
    "def process_file(file_content):\n",
    "    \"\"\"\n",
    "    Processes the content of a file.\n",
    "    Assumes each row is of the format:\n",
    "      event type : Feynman diagram : amplitude : squared amplitude\n",
    "    Returns a list of dictionaries with tokenized amplitude and squared amplitude,\n",
    "    along with the raw event type and Feynman diagram for reference.\n",
    "    \"\"\"\n",
    "    processed_rows = []\n",
    "    for line in file_content.strip().splitlines():\n",
    "        # Skip empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        # Split the line into four parts by \" : \"\n",
    "        parts = [p.strip() for p in line.split(\" : \")]\n",
    "        if len(parts) != 4:\n",
    "            print(f\"Skipping malformed line: {line}\")\n",
    "            continue\n",
    "        event_type, feynman_diag, amplitude, squared_amplitude = parts\n",
    "\n",
    "        # Normalize indices for amplitude and squared amplitude\n",
    "        amplitude_norm = normalize_indices(amplitude)\n",
    "        squared_amplitude_norm = normalize_indices(squared_amplitude)\n",
    "\n",
    "        # Tokenize amplitude and squared amplitude\n",
    "        amplitude_tokens = tokenize(amplitude_norm)\n",
    "        squared_amplitude_tokens = tokenize(squared_amplitude_norm)\n",
    "\n",
    "        processed_rows.append({\n",
    "            \"event_type\": event_type,\n",
    "            \"feynman_diag\": feynman_diag,\n",
    "            \"amplitude_tokens\": amplitude_tokens,\n",
    "            \"squared_amplitude_tokens\": squared_amplitude_tokens\n",
    "        })\n",
    "    return processed_rows\n",
    "\n",
    "def load_and_process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all text files in the provided folder and processes them.\n",
    "    Returns the aggregated list of processed rows.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    folder = Path(folder_path)\n",
    "    # Glob for .txt files in the folder\n",
    "    text_files = list(folder.glob(\"*.txt\"))\n",
    "    for file_path in text_files:\n",
    "        with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            file_data = process_file(content)\n",
    "            all_data.extend(file_data)\n",
    "    return all_data\n",
    "\n",
    "def split_dataset(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Shuffles and splits the data into train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    random.shuffle(data)\n",
    "    total = len(data)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = train_end + int(total * val_ratio)\n",
    "    train_set = data[:train_end]\n",
    "    val_set = data[train_end:val_end]\n",
    "    test_set = data[val_end:]\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "# Main processing\n",
    "folder_path = \"/kaggle/input/squared-amplitude/SYMBA - Test Data\"  # Path to the folder containing extracted text files.\n",
    "all_processed_data = load_and_process_folder(folder_path)\n",
    "print(f\"Total examples processed: {len(all_processed_data)}\")\n",
    "\n",
    "# Split the dataset\n",
    "train_set, val_set, test_set = split_dataset(all_processed_data)\n",
    "print(f\"Train set: {len(train_set)} examples\")\n",
    "print(f\"Validation set: {len(val_set)} examples\")\n",
    "print(f\"Test set: {len(test_set)} examples\")\n",
    "\n",
    "# Optionally, save the splits to disk as JSON for further processing\n",
    "output_dir = Path(\"processed_data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "with open(output_dir / \"train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_set, f, indent=2)\n",
    "with open(output_dir / \"val.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_set, f, indent=2)\n",
    "with open(output_dir / \"test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_set, f, indent=2)\n",
    "\n",
    "print(\"Data processing and splitting completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSplUdBZERJK"
   },
   "source": [
    "### Common Task 2 BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-25T00:12:57.561023Z",
     "iopub.status.busy": "2025-03-25T00:12:57.560695Z",
     "iopub.status.idle": "2025-03-25T07:29:38.784493Z",
     "shell.execute_reply": "2025-03-25T07:29:38.782750Z",
     "shell.execute_reply.started": "2025-03-25T00:12:57.560997Z"
    },
    "id": "TV4jkaPDEFen",
    "outputId": "b9cd69e3-5850-40bc-aee0-eb6043a6028e",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32828\n",
      "Epoch 1: Train Loss = 3.8072, Val Loss = 3.3031\n",
      "Epoch 2: Train Loss = 3.1891, Val Loss = 2.9602\n",
      "Epoch 3: Train Loss = 2.9058, Val Loss = 2.6946\n",
      "Epoch 4: Train Loss = 2.6199, Val Loss = 2.4222\n",
      "Epoch 5: Train Loss = 2.3676, Val Loss = 2.1747\n",
      "Epoch 6: Train Loss = 2.1512, Val Loss = 1.9753\n",
      "Epoch 7: Train Loss = 1.9693, Val Loss = 1.7926\n",
      "Epoch 8: Train Loss = 1.8327, Val Loss = 1.6760\n",
      "Epoch 9: Train Loss = 1.7288, Val Loss = 1.6019\n",
      "Epoch 10: Train Loss = 1.6275, Val Loss = 1.5048\n",
      "Epoch 11: Train Loss = 1.5547, Val Loss = 1.4133\n",
      "Epoch 12: Train Loss = 1.4963, Val Loss = 1.3948\n",
      "Epoch 13: Train Loss = 1.4372, Val Loss = 1.3471\n",
      "Epoch 14: Train Loss = 1.3858, Val Loss = 1.2808\n",
      "Epoch 15: Train Loss = 1.3285, Val Loss = 1.2444\n",
      "Epoch 16: Train Loss = 1.2958, Val Loss = 1.1986\n",
      "Epoch 17: Train Loss = 1.2626, Val Loss = 1.1810\n",
      "Epoch 18: Train Loss = 1.2140, Val Loss = 1.1744\n",
      "Epoch 19: Train Loss = 1.1888, Val Loss = 1.1327\n",
      "Epoch 20: Train Loss = 1.1723, Val Loss = 1.1175\n",
      "Epoch 21: Train Loss = 1.1313, Val Loss = 1.0882\n",
      "Epoch 22: Train Loss = 1.1179, Val Loss = 1.0899\n",
      "Epoch 23: Train Loss = 1.0969, Val Loss = 1.0619\n",
      "Epoch 24: Train Loss = 1.0694, Val Loss = 1.0574\n",
      "Epoch 25: Train Loss = 1.0678, Val Loss = 1.0299\n",
      "Epoch 26: Train Loss = 1.0394, Val Loss = 1.0113\n",
      "Epoch 27: Train Loss = 1.0343, Val Loss = 1.0466\n",
      "Epoch 28: Train Loss = 1.0240, Val Loss = 1.0235\n",
      "Epoch 29: Train Loss = 1.0098, Val Loss = 1.0760\n",
      "Epoch 30: Train Loss = 0.9958, Val Loss = 1.0107\n",
      "Epoch 31: Train Loss = 1.0011, Val Loss = 1.0239\n",
      "Epoch 32: Train Loss = 0.9792, Val Loss = 1.0347\n",
      "Epoch 33: Train Loss = 0.9644, Val Loss = 0.9889\n",
      "Epoch 34: Train Loss = 0.9506, Val Loss = 0.9803\n",
      "Epoch 35: Train Loss = 0.9426, Val Loss = 1.0217\n",
      "Epoch 36: Train Loss = 0.9339, Val Loss = 0.9922\n",
      "Epoch 37: Train Loss = 0.9326, Val Loss = 1.1137\n",
      "Epoch 38: Train Loss = 0.9188, Val Loss = 0.9823\n",
      "Epoch 39: Train Loss = 0.9517, Val Loss = 1.0387\n",
      "Epoch 40: Train Loss = 0.9021, Val Loss = 1.0329\n",
      "Epoch 41: Train Loss = 0.9002, Val Loss = 1.0138\n",
      "Epoch 42: Train Loss = 0.8949, Val Loss = 0.9636\n",
      "Epoch 43: Train Loss = 0.8699, Val Loss = 0.9863\n",
      "Epoch 44: Train Loss = 0.8860, Val Loss = 0.9970\n",
      "Epoch 45: Train Loss = 0.8804, Val Loss = 1.0970\n",
      "Epoch 46: Train Loss = 0.8738, Val Loss = 0.9575\n",
      "Epoch 47: Train Loss = 0.8486, Val Loss = 1.0055\n",
      "Epoch 48: Train Loss = 0.8783, Val Loss = 1.2950\n",
      "Epoch 49: Train Loss = 0.9081, Val Loss = 1.0124\n",
      "Epoch 50: Train Loss = 0.8502, Val Loss = 0.9849\n",
      "Epoch 51: Train Loss = 0.8222, Val Loss = 0.9885\n",
      "Epoch 52: Train Loss = 0.8184, Val Loss = 0.9949\n",
      "Epoch 53: Train Loss = 0.8243, Val Loss = 1.0107\n",
      "Epoch 54: Train Loss = 0.8336, Val Loss = 1.0795\n",
      "Epoch 55: Train Loss = 0.8309, Val Loss = 0.9856\n",
      "Epoch 56: Train Loss = 0.7971, Val Loss = 0.9349\n",
      "Epoch 57: Train Loss = 0.7934, Val Loss = 0.9858\n",
      "Epoch 58: Train Loss = 0.8361, Val Loss = 0.9527\n",
      "Epoch 59: Train Loss = 0.8059, Val Loss = 0.9789\n",
      "Epoch 60: Train Loss = 0.7867, Val Loss = 0.9960\n",
      "Epoch 61: Train Loss = 0.7908, Val Loss = 0.9924\n",
      "Epoch 62: Train Loss = 0.7770, Val Loss = 1.0233\n",
      "Epoch 63: Train Loss = 0.7852, Val Loss = 0.9737\n",
      "Epoch 64: Train Loss = 0.7713, Val Loss = 0.9793\n",
      "Epoch 65: Train Loss = 0.7843, Val Loss = 0.9656\n",
      "Epoch 66: Train Loss = 0.7785, Val Loss = 1.0095\n",
      "Epoch 67: Train Loss = 0.7484, Val Loss = 0.9860\n",
      "Epoch 68: Train Loss = 0.7658, Val Loss = 1.0851\n",
      "Epoch 69: Train Loss = 0.7697, Val Loss = 1.1105\n",
      "Epoch 70: Train Loss = 0.7675, Val Loss = 0.9636\n",
      "Epoch 71: Train Loss = 0.7384, Val Loss = 0.9997\n",
      "Epoch 72: Train Loss = 0.7814, Val Loss = 0.9942\n",
      "Epoch 73: Train Loss = 0.7532, Val Loss = 1.0424\n",
      "Epoch 74: Train Loss = 0.7454, Val Loss = 1.1294\n",
      "Epoch 75: Train Loss = 0.7644, Val Loss = 1.0563\n",
      "Epoch 76: Train Loss = 0.7422, Val Loss = 0.9817\n",
      "Epoch 77: Train Loss = 0.7507, Val Loss = 0.9753\n",
      "Epoch 78: Train Loss = 0.7422, Val Loss = 1.0671\n",
      "Epoch 79: Train Loss = 0.7227, Val Loss = 1.0250\n",
      "Epoch 80: Train Loss = 0.7251, Val Loss = 1.0665\n",
      "Epoch 81: Train Loss = 0.7477, Val Loss = 1.0579\n",
      "Epoch 82: Train Loss = 0.7397, Val Loss = 1.0569\n",
      "Epoch 83: Train Loss = 0.7394, Val Loss = 1.0549\n",
      "Epoch 84: Train Loss = 0.7237, Val Loss = 1.0458\n",
      "Epoch 85: Train Loss = 0.7254, Val Loss = 1.0303\n",
      "Epoch 86: Train Loss = 0.7368, Val Loss = 1.1270\n",
      "Epoch 87: Train Loss = 0.7196, Val Loss = 0.9854\n",
      "Epoch 88: Train Loss = 0.7050, Val Loss = 1.0604\n",
      "Epoch 89: Train Loss = 0.7093, Val Loss = 1.1601\n",
      "Epoch 90: Train Loss = 0.7112, Val Loss = 1.0349\n",
      "Epoch 91: Train Loss = 0.7135, Val Loss = 1.0083\n",
      "Epoch 92: Train Loss = 0.7110, Val Loss = 0.9729\n",
      "Epoch 93: Train Loss = 0.7170, Val Loss = 1.0933\n",
      "Epoch 94: Train Loss = 0.7077, Val Loss = 1.1528\n",
      "Epoch 95: Train Loss = 0.7026, Val Loss = 1.0268\n",
      "Epoch 96: Train Loss = 0.7046, Val Loss = 1.0376\n",
      "Epoch 97: Train Loss = 0.7020, Val Loss = 1.0480\n",
      "Epoch 98: Train Loss = 0.7034, Val Loss = 0.9541\n",
      "Epoch 99: Train Loss = 0.6967, Val Loss = 1.1014\n",
      "Epoch 100: Train Loss = 0.7042, Val Loss = 1.0732\n",
      "Epoch 101: Train Loss = 0.7049, Val Loss = 1.0482\n",
      "Epoch 102: Train Loss = 0.6848, Val Loss = 1.0295\n",
      "Epoch 103: Train Loss = 0.6889, Val Loss = 0.9620\n",
      "Epoch 104: Train Loss = 0.6769, Val Loss = 1.0532\n",
      "Epoch 105: Train Loss = 0.6800, Val Loss = 0.9761\n",
      "Epoch 106: Train Loss = 0.6924, Val Loss = 1.0208\n",
      "Epoch 107: Train Loss = 0.6889, Val Loss = 1.1637\n",
      "Epoch 108: Train Loss = 0.6878, Val Loss = 1.0504\n",
      "Epoch 109: Train Loss = 0.6855, Val Loss = 1.0422\n",
      "Epoch 110: Train Loss = 0.6879, Val Loss = 0.9869\n",
      "Epoch 111: Train Loss = 0.6728, Val Loss = 1.0703\n",
      "Epoch 112: Train Loss = 0.6878, Val Loss = 1.1516\n",
      "Epoch 113: Train Loss = 0.6799, Val Loss = 1.0569\n",
      "Epoch 114: Train Loss = 0.6653, Val Loss = 1.0115\n",
      "Epoch 115: Train Loss = 0.6695, Val Loss = 1.0145\n",
      "Epoch 116: Train Loss = 0.6714, Val Loss = 0.9631\n",
      "Epoch 117: Train Loss = 0.6674, Val Loss = 1.2844\n",
      "Epoch 118: Train Loss = 0.6840, Val Loss = 0.9437\n",
      "Epoch 119: Train Loss = 0.6799, Val Loss = 0.9889\n",
      "Epoch 120: Train Loss = 0.6656, Val Loss = 0.9477\n",
      "Epoch 121: Train Loss = 0.6712, Val Loss = 1.0072\n",
      "Epoch 122: Train Loss = 0.6658, Val Loss = 0.9684\n",
      "Epoch 123: Train Loss = 0.6557, Val Loss = 1.0000\n",
      "Epoch 124: Train Loss = 0.6772, Val Loss = 1.0505\n",
      "Epoch 125: Train Loss = 0.6698, Val Loss = 1.0482\n",
      "Epoch 126: Train Loss = 0.6608, Val Loss = 0.9581\n",
      "Epoch 127: Train Loss = 0.6588, Val Loss = 1.0283\n",
      "Epoch 128: Train Loss = 0.6487, Val Loss = 1.0739\n",
      "Epoch 129: Train Loss = 0.6482, Val Loss = 1.0194\n",
      "Epoch 130: Train Loss = 0.6689, Val Loss = 1.0829\n",
      "Epoch 131: Train Loss = 0.6586, Val Loss = 0.9835\n",
      "Epoch 132: Train Loss = 0.6649, Val Loss = 1.0442\n",
      "Epoch 133: Train Loss = 0.6569, Val Loss = 1.0408\n",
      "Epoch 134: Train Loss = 0.6513, Val Loss = 0.9818\n",
      "Epoch 135: Train Loss = 0.6481, Val Loss = 1.0570\n",
      "Epoch 136: Train Loss = 0.6475, Val Loss = 1.1032\n",
      "Epoch 137: Train Loss = 0.6450, Val Loss = 1.0080\n",
      "Epoch 138: Train Loss = 0.6320, Val Loss = 0.9727\n",
      "Epoch 139: Train Loss = 0.6480, Val Loss = 1.0478\n",
      "Epoch 140: Train Loss = 0.6504, Val Loss = 1.1291\n",
      "Epoch 141: Train Loss = 0.6315, Val Loss = 1.0499\n",
      "Epoch 142: Train Loss = 0.6521, Val Loss = 1.0959\n",
      "Epoch 143: Train Loss = 0.6391, Val Loss = 0.9750\n",
      "Epoch 144: Train Loss = 0.6566, Val Loss = 1.0839\n",
      "Epoch 145: Train Loss = 0.6299, Val Loss = 1.1215\n",
      "Epoch 146: Train Loss = 0.6263, Val Loss = 0.9410\n",
      "Epoch 147: Train Loss = 0.6519, Val Loss = 1.1804\n",
      "Epoch 148: Train Loss = 0.6333, Val Loss = 1.0339\n",
      "Epoch 149: Train Loss = 0.6182, Val Loss = 1.0677\n",
      "Epoch 150: Train Loss = 0.6321, Val Loss = 1.1583\n",
      "Epoch 151: Train Loss = 0.6391, Val Loss = 1.0630\n",
      "Epoch 152: Train Loss = 0.6294, Val Loss = 1.0358\n",
      "Epoch 153: Train Loss = 0.6387, Val Loss = 1.0421\n",
      "Epoch 154: Train Loss = 0.6274, Val Loss = 0.9256\n",
      "Epoch 155: Train Loss = 0.6159, Val Loss = 0.9719\n",
      "Epoch 156: Train Loss = 0.6381, Val Loss = 1.1743\n",
      "Epoch 157: Train Loss = 0.6211, Val Loss = 0.9948\n",
      "Epoch 158: Train Loss = 0.6251, Val Loss = 1.1079\n",
      "Epoch 159: Train Loss = 0.6287, Val Loss = 1.0199\n",
      "Epoch 160: Train Loss = 0.6166, Val Loss = 1.0314\n",
      "Epoch 161: Train Loss = 0.6374, Val Loss = 1.0564\n",
      "Epoch 162: Train Loss = 0.6366, Val Loss = 1.2600\n",
      "Epoch 163: Train Loss = 0.6262, Val Loss = 1.0107\n",
      "Epoch 164: Train Loss = 0.6250, Val Loss = 1.0330\n",
      "Epoch 165: Train Loss = 0.6182, Val Loss = 1.0753\n",
      "Epoch 166: Train Loss = 0.6018, Val Loss = 1.0778\n",
      "Epoch 167: Train Loss = 0.6194, Val Loss = 0.9980\n",
      "Epoch 168: Train Loss = 0.6068, Val Loss = 1.0579\n",
      "Epoch 169: Train Loss = 0.5995, Val Loss = 1.0189\n",
      "Epoch 170: Train Loss = 0.6495, Val Loss = 1.1417\n",
      "Epoch 171: Train Loss = 0.6257, Val Loss = 1.0797\n",
      "Epoch 172: Train Loss = 0.6132, Val Loss = 1.0911\n",
      "Epoch 173: Train Loss = 0.6145, Val Loss = 1.1143\n",
      "Epoch 174: Train Loss = 0.6192, Val Loss = 1.0090\n",
      "Epoch 175: Train Loss = 0.6066, Val Loss = 1.0108\n",
      "Epoch 176: Train Loss = 0.6088, Val Loss = 1.1527\n",
      "Epoch 177: Train Loss = 0.6095, Val Loss = 1.0448\n",
      "Epoch 178: Train Loss = 0.6214, Val Loss = 1.1104\n",
      "Epoch 179: Train Loss = 0.5978, Val Loss = 1.0905\n",
      "Epoch 180: Train Loss = 0.6157, Val Loss = 1.0405\n",
      "Epoch 181: Train Loss = 0.6096, Val Loss = 1.1146\n",
      "Epoch 182: Train Loss = 0.6027, Val Loss = 0.9953\n",
      "Epoch 183: Train Loss = 0.5943, Val Loss = 1.0207\n",
      "Epoch 184: Train Loss = 0.6047, Val Loss = 1.1848\n",
      "Epoch 185: Train Loss = 0.6169, Val Loss = 1.0396\n",
      "Epoch 186: Train Loss = 0.6213, Val Loss = 1.1220\n",
      "Epoch 187: Train Loss = 0.5877, Val Loss = 1.0198\n",
      "Epoch 188: Train Loss = 0.5877, Val Loss = 1.0541\n",
      "Epoch 189: Train Loss = 0.5930, Val Loss = 1.0752\n",
      "Epoch 190: Train Loss = 0.6155, Val Loss = 0.9732\n",
      "Epoch 191: Train Loss = 0.6046, Val Loss = 1.0237\n",
      "Epoch 192: Train Loss = 0.5973, Val Loss = 1.0140\n",
      "Epoch 193: Train Loss = 0.5932, Val Loss = 0.9790\n",
      "Epoch 194: Train Loss = 0.5848, Val Loss = 0.9903\n",
      "Epoch 195: Train Loss = 0.6053, Val Loss = 1.0025\n",
      "Epoch 196: Train Loss = 0.5991, Val Loss = 0.9967\n",
      "Epoch 197: Train Loss = 0.5974, Val Loss = 1.0468\n",
      "Epoch 198: Train Loss = 0.6034, Val Loss = 1.1686\n",
      "Epoch 199: Train Loss = 0.5886, Val Loss = 1.0555\n",
      "Epoch 200: Train Loss = 0.5825, Val Loss = 0.9804\n",
      "Epoch 201: Train Loss = 0.6019, Val Loss = 1.1277\n",
      "Epoch 202: Train Loss = 0.5971, Val Loss = 1.0569\n",
      "Epoch 203: Train Loss = 0.5945, Val Loss = 0.9984\n",
      "Epoch 204: Train Loss = 0.5767, Val Loss = 1.1484\n",
      "Epoch 205: Train Loss = 0.6994, Val Loss = 0.9485\n",
      "Epoch 206: Train Loss = 0.5885, Val Loss = 1.1334\n",
      "Epoch 207: Train Loss = 0.6000, Val Loss = 1.2180\n",
      "Epoch 208: Train Loss = 0.5937, Val Loss = 1.1018\n",
      "Epoch 209: Train Loss = 0.5857, Val Loss = 0.9983\n",
      "Epoch 210: Train Loss = 0.5976, Val Loss = 1.0346\n",
      "Epoch 211: Train Loss = 0.5916, Val Loss = 1.1734\n",
      "Epoch 212: Train Loss = 0.5894, Val Loss = 1.1847\n",
      "Epoch 213: Train Loss = 0.5991, Val Loss = 1.0712\n",
      "Epoch 214: Train Loss = 0.5930, Val Loss = 1.1016\n",
      "Epoch 215: Train Loss = 0.5834, Val Loss = 1.0543\n",
      "Epoch 216: Train Loss = 0.5704, Val Loss = 1.0836\n",
      "Epoch 217: Train Loss = 0.5941, Val Loss = 1.0582\n",
      "Epoch 218: Train Loss = 0.5761, Val Loss = 1.0358\n",
      "Epoch 219: Train Loss = 0.5845, Val Loss = 1.1165\n",
      "Epoch 220: Train Loss = 0.5863, Val Loss = 1.0983\n",
      "Epoch 221: Train Loss = 0.5670, Val Loss = 1.0551\n",
      "Epoch 222: Train Loss = 0.5782, Val Loss = 1.0638\n",
      "Epoch 223: Train Loss = 0.5773, Val Loss = 1.0624\n",
      "Epoch 224: Train Loss = 0.5733, Val Loss = 1.0678\n",
      "Epoch 225: Train Loss = 0.5868, Val Loss = 1.0379\n",
      "Epoch 226: Train Loss = 0.5731, Val Loss = 1.1415\n",
      "Epoch 227: Train Loss = 0.5839, Val Loss = 1.0428\n",
      "Epoch 228: Train Loss = 0.5942, Val Loss = 1.0567\n",
      "Epoch 229: Train Loss = 0.5817, Val Loss = 1.1171\n",
      "Epoch 230: Train Loss = 0.5723, Val Loss = 0.9962\n",
      "Epoch 231: Train Loss = 0.5627, Val Loss = 1.0089\n",
      "Epoch 232: Train Loss = 0.5617, Val Loss = 1.0434\n",
      "Epoch 233: Train Loss = 0.5783, Val Loss = 1.0788\n",
      "Epoch 234: Train Loss = 0.5569, Val Loss = 1.1072\n",
      "Epoch 235: Train Loss = 0.5782, Val Loss = 1.0041\n",
      "Epoch 236: Train Loss = 0.5658, Val Loss = 1.0226\n",
      "Epoch 237: Train Loss = 0.5697, Val Loss = 1.1539\n",
      "Epoch 238: Train Loss = 0.5766, Val Loss = 1.1089\n",
      "Epoch 239: Train Loss = 0.5658, Val Loss = 1.0053\n",
      "Epoch 240: Train Loss = 0.5679, Val Loss = 0.9683\n",
      "Epoch 241: Train Loss = 0.5656, Val Loss = 1.0240\n",
      "Epoch 242: Train Loss = 0.5755, Val Loss = 0.9274\n",
      "Epoch 243: Train Loss = 0.5712, Val Loss = 0.9999\n",
      "Epoch 244: Train Loss = 0.5649, Val Loss = 1.0525\n",
      "Epoch 245: Train Loss = 0.5731, Val Loss = 1.0067\n",
      "Epoch 246: Train Loss = 0.5722, Val Loss = 1.0627\n",
      "Epoch 247: Train Loss = 0.5713, Val Loss = 0.9672\n",
      "Epoch 248: Train Loss = 0.5664, Val Loss = 0.9551\n",
      "Epoch 249: Train Loss = 0.5517, Val Loss = 1.0135\n",
      "Epoch 250: Train Loss = 0.5623, Val Loss = 1.0393\n",
      "Epoch 251: Train Loss = 0.5658, Val Loss = 0.9789\n",
      "Epoch 252: Train Loss = 0.5573, Val Loss = 0.9736\n",
      "Epoch 253: Train Loss = 0.5725, Val Loss = 1.1188\n",
      "Epoch 254: Train Loss = 0.5674, Val Loss = 1.0992\n",
      "Epoch 255: Train Loss = 0.5642, Val Loss = 1.1046\n",
      "Epoch 256: Train Loss = 0.5547, Val Loss = 1.0726\n",
      "Epoch 257: Train Loss = 0.5634, Val Loss = 1.0358\n",
      "Epoch 258: Train Loss = 0.5683, Val Loss = 1.0230\n",
      "Epoch 259: Train Loss = 0.5650, Val Loss = 1.0993\n",
      "Epoch 260: Train Loss = 0.5624, Val Loss = 1.0709\n",
      "Epoch 261: Train Loss = 0.5663, Val Loss = 1.0970\n",
      "Epoch 262: Train Loss = 0.5479, Val Loss = 1.0607\n",
      "Epoch 263: Train Loss = 0.5644, Val Loss = 0.9776\n",
      "Epoch 264: Train Loss = 0.5714, Val Loss = 1.1098\n",
      "Epoch 265: Train Loss = 0.5542, Val Loss = 1.0691\n",
      "Epoch 266: Train Loss = 0.5771, Val Loss = 1.0703\n",
      "Epoch 267: Train Loss = 0.5485, Val Loss = 1.1261\n",
      "Epoch 268: Train Loss = 0.5523, Val Loss = 0.9575\n",
      "Epoch 269: Train Loss = 0.5657, Val Loss = 1.0907\n",
      "Epoch 270: Train Loss = 0.5818, Val Loss = 1.1195\n",
      "Epoch 271: Train Loss = 0.5559, Val Loss = 1.0530\n",
      "Epoch 272: Train Loss = 0.5441, Val Loss = 1.1442\n",
      "Epoch 273: Train Loss = 0.5519, Val Loss = 1.0906\n",
      "Epoch 274: Train Loss = 0.5546, Val Loss = 1.1106\n",
      "Epoch 275: Train Loss = 0.5510, Val Loss = 1.1976\n",
      "Epoch 276: Train Loss = 0.5546, Val Loss = 1.1587\n",
      "Epoch 277: Train Loss = 0.5692, Val Loss = 1.1377\n",
      "Epoch 278: Train Loss = 0.5499, Val Loss = 1.0828\n",
      "Epoch 279: Train Loss = 0.5444, Val Loss = 0.9923\n",
      "Epoch 280: Train Loss = 0.5609, Val Loss = 1.0290\n",
      "Epoch 281: Train Loss = 0.5569, Val Loss = 1.0617\n",
      "Epoch 282: Train Loss = 0.5406, Val Loss = 1.0410\n",
      "Epoch 283: Train Loss = 0.5771, Val Loss = 1.1218\n",
      "Epoch 284: Train Loss = 0.5512, Val Loss = 1.0385\n",
      "Epoch 285: Train Loss = 0.5523, Val Loss = 1.1516\n",
      "Epoch 286: Train Loss = 0.5571, Val Loss = 1.0794\n",
      "Epoch 287: Train Loss = 0.5583, Val Loss = 0.9850\n",
      "Epoch 288: Train Loss = 0.5495, Val Loss = 1.1776\n",
      "Epoch 289: Train Loss = 0.5582, Val Loss = 1.0727\n",
      "Epoch 290: Train Loss = 0.5617, Val Loss = 1.1220\n",
      "Epoch 291: Train Loss = 0.5613, Val Loss = 1.1406\n",
      "Epoch 292: Train Loss = 0.5501, Val Loss = 1.0075\n",
      "Epoch 293: Train Loss = 0.5539, Val Loss = 1.0862\n",
      "Epoch 294: Train Loss = 0.5492, Val Loss = 1.1402\n",
      "Epoch 295: Train Loss = 0.5676, Val Loss = 1.0876\n",
      "Epoch 296: Train Loss = 0.5550, Val Loss = 1.0740\n",
      "Epoch 297: Train Loss = 0.5671, Val Loss = 1.0707\n",
      "Epoch 298: Train Loss = 0.5521, Val Loss = 1.1648\n",
      "Epoch 299: Train Loss = 0.5529, Val Loss = 1.0811\n",
      "Epoch 300: Train Loss = 0.5428, Val Loss = 1.1734\n",
      "Epoch 301: Train Loss = 0.5553, Val Loss = 1.2085\n",
      "Epoch 302: Train Loss = 0.7545, Val Loss = 2.7620\n",
      "Epoch 303: Train Loss = 1.6624, Val Loss = 1.0347\n",
      "Epoch 304: Train Loss = 0.5564, Val Loss = 1.0073\n",
      "Epoch 305: Train Loss = 0.5481, Val Loss = 1.0219\n",
      "Epoch 306: Train Loss = 0.5396, Val Loss = 1.0720\n",
      "Epoch 307: Train Loss = 0.5480, Val Loss = 1.0153\n",
      "Epoch 308: Train Loss = 0.5583, Val Loss = 1.0832\n",
      "Epoch 309: Train Loss = 0.5451, Val Loss = 1.1193\n",
      "Epoch 310: Train Loss = 0.5406, Val Loss = 1.1083\n",
      "Epoch 311: Train Loss = 0.5568, Val Loss = 1.1188\n",
      "Epoch 312: Train Loss = 0.5439, Val Loss = 1.0499\n",
      "Epoch 313: Train Loss = 0.5528, Val Loss = 1.0572\n",
      "Epoch 314: Train Loss = 0.5383, Val Loss = 1.0562\n",
      "Epoch 315: Train Loss = 0.5339, Val Loss = 1.1546\n",
      "Epoch 316: Train Loss = 0.5478, Val Loss = 1.1037\n",
      "Epoch 317: Train Loss = 0.5404, Val Loss = 1.0176\n",
      "Epoch 318: Train Loss = 0.5820, Val Loss = 1.1853\n",
      "Epoch 319: Train Loss = 0.5513, Val Loss = 1.0115\n",
      "Epoch 320: Train Loss = 0.5652, Val Loss = 1.2366\n",
      "Epoch 321: Train Loss = 0.5493, Val Loss = 1.2767\n",
      "Epoch 322: Train Loss = 0.5488, Val Loss = 1.1114\n",
      "Epoch 323: Train Loss = 0.5365, Val Loss = 1.1066\n",
      "Epoch 324: Train Loss = 0.5287, Val Loss = 1.1263\n",
      "Epoch 325: Train Loss = 0.5382, Val Loss = 1.0758\n",
      "Epoch 326: Train Loss = 0.5500, Val Loss = 1.1137\n",
      "Epoch 327: Train Loss = 0.5652, Val Loss = 1.1481\n",
      "Epoch 328: Train Loss = 0.5547, Val Loss = 1.0988\n",
      "Epoch 329: Train Loss = 0.5353, Val Loss = 1.0989\n",
      "Epoch 330: Train Loss = 0.5489, Val Loss = 1.0821\n",
      "Epoch 331: Train Loss = 0.5479, Val Loss = 0.9995\n",
      "Epoch 332: Train Loss = 0.5593, Val Loss = 1.0311\n",
      "Epoch 333: Train Loss = 0.5468, Val Loss = 0.9979\n",
      "Epoch 334: Train Loss = 0.5461, Val Loss = 1.0211\n",
      "Epoch 335: Train Loss = 0.5535, Val Loss = 1.1577\n",
      "Epoch 336: Train Loss = 0.5546, Val Loss = 1.1309\n",
      "Epoch 337: Train Loss = 0.5416, Val Loss = 1.0393\n",
      "Epoch 338: Train Loss = 0.5400, Val Loss = 1.0947\n",
      "Epoch 339: Train Loss = 0.5549, Val Loss = 1.1630\n",
      "Epoch 340: Train Loss = 0.5457, Val Loss = 1.0791\n",
      "Epoch 341: Train Loss = 0.5388, Val Loss = 1.1076\n",
      "Epoch 342: Train Loss = 0.5301, Val Loss = 1.2963\n",
      "Epoch 343: Train Loss = 0.5569, Val Loss = 1.1252\n",
      "Epoch 344: Train Loss = 0.5486, Val Loss = 1.1390\n",
      "Epoch 345: Train Loss = 0.5321, Val Loss = 1.0820\n",
      "Epoch 346: Train Loss = 0.5347, Val Loss = 1.0600\n",
      "Epoch 347: Train Loss = 0.5318, Val Loss = 1.0944\n",
      "Epoch 348: Train Loss = 0.5387, Val Loss = 1.1491\n",
      "Epoch 349: Train Loss = 0.5468, Val Loss = 1.2968\n",
      "Epoch 350: Train Loss = 0.5598, Val Loss = 1.2945\n",
      "Epoch 351: Train Loss = 0.5334, Val Loss = 1.2136\n",
      "Epoch 352: Train Loss = 0.5417, Val Loss = 1.1134\n",
      "Epoch 353: Train Loss = 0.5395, Val Loss = 1.2733\n",
      "Epoch 354: Train Loss = 0.5315, Val Loss = 1.0926\n",
      "Epoch 355: Train Loss = 0.5355, Val Loss = 1.1632\n",
      "Epoch 356: Train Loss = 0.5295, Val Loss = 1.0078\n",
      "Epoch 357: Train Loss = 0.5461, Val Loss = 1.0798\n",
      "Epoch 358: Train Loss = 0.5511, Val Loss = 1.0946\n",
      "Epoch 359: Train Loss = 0.5340, Val Loss = 1.0579\n",
      "Epoch 360: Train Loss = 0.5478, Val Loss = 1.2747\n",
      "Epoch 361: Train Loss = 0.5266, Val Loss = 1.1360\n",
      "Epoch 362: Train Loss = 0.5314, Val Loss = 1.1264\n",
      "Epoch 363: Train Loss = 0.5308, Val Loss = 1.1230\n",
      "Epoch 364: Train Loss = 0.5509, Val Loss = 1.0801\n",
      "Epoch 365: Train Loss = 0.5296, Val Loss = 1.1377\n",
      "Epoch 366: Train Loss = 0.5519, Val Loss = 1.1366\n",
      "Epoch 367: Train Loss = 0.5567, Val Loss = 1.1210\n",
      "Epoch 368: Train Loss = 0.5442, Val Loss = 1.0836\n",
      "Epoch 369: Train Loss = 0.5608, Val Loss = 1.0909\n",
      "Epoch 370: Train Loss = 0.5509, Val Loss = 1.1631\n",
      "Epoch 371: Train Loss = 0.5368, Val Loss = 1.1147\n",
      "Epoch 372: Train Loss = 0.5320, Val Loss = 1.0573\n",
      "Epoch 373: Train Loss = 0.5337, Val Loss = 1.3392\n",
      "Epoch 374: Train Loss = 0.5476, Val Loss = 1.1559\n",
      "Epoch 375: Train Loss = 0.5227, Val Loss = 1.1306\n",
      "Epoch 376: Train Loss = 0.5299, Val Loss = 1.0858\n",
      "Epoch 377: Train Loss = 0.5265, Val Loss = 1.1888\n",
      "Epoch 378: Train Loss = 0.5298, Val Loss = 1.0750\n",
      "Epoch 379: Train Loss = 0.5220, Val Loss = 1.2698\n",
      "Epoch 380: Train Loss = 0.5367, Val Loss = 1.1431\n",
      "Epoch 381: Train Loss = 0.5325, Val Loss = 1.1099\n",
      "Epoch 382: Train Loss = 0.5289, Val Loss = 1.1325\n",
      "Epoch 383: Train Loss = 0.5367, Val Loss = 1.0387\n",
      "Epoch 384: Train Loss = 0.5244, Val Loss = 1.2145\n",
      "Epoch 385: Train Loss = 0.5291, Val Loss = 0.9996\n",
      "Epoch 386: Train Loss = 0.5263, Val Loss = 1.0381\n",
      "Epoch 387: Train Loss = 0.5230, Val Loss = 1.0631\n",
      "Epoch 388: Train Loss = 0.5317, Val Loss = 1.2921\n",
      "Epoch 389: Train Loss = 0.5376, Val Loss = 1.0775\n",
      "Epoch 390: Train Loss = 0.5419, Val Loss = 1.3318\n",
      "Epoch 391: Train Loss = 0.5276, Val Loss = 1.1182\n",
      "Epoch 392: Train Loss = 0.5153, Val Loss = 1.0983\n",
      "Epoch 393: Train Loss = 0.5216, Val Loss = 1.2041\n",
      "Epoch 394: Train Loss = 0.5413, Val Loss = 1.1062\n",
      "Epoch 395: Train Loss = 0.5089, Val Loss = 1.0689\n",
      "Epoch 396: Train Loss = 0.5274, Val Loss = 1.0098\n",
      "Epoch 397: Train Loss = 0.5413, Val Loss = 1.2106\n",
      "Epoch 398: Train Loss = 0.5279, Val Loss = 1.0898\n",
      "Epoch 399: Train Loss = 0.5246, Val Loss = 1.1137\n",
      "Epoch 400: Train Loss = 0.5278, Val Loss = 1.1506\n",
      "Epoch 401: Train Loss = 0.5306, Val Loss = 1.0169\n",
      "Epoch 402: Train Loss = 0.5325, Val Loss = 1.0839\n",
      "Epoch 403: Train Loss = 0.5240, Val Loss = 1.0534\n",
      "Epoch 404: Train Loss = 0.5163, Val Loss = 1.0232\n",
      "Epoch 405: Train Loss = 0.5689, Val Loss = 1.1710\n",
      "Epoch 406: Train Loss = 0.5207, Val Loss = 1.0722\n",
      "Epoch 407: Train Loss = 0.5121, Val Loss = 1.1025\n",
      "Epoch 408: Train Loss = 0.5288, Val Loss = 1.0179\n",
      "Epoch 409: Train Loss = 0.5256, Val Loss = 1.0829\n",
      "Epoch 410: Train Loss = 0.5266, Val Loss = 0.9794\n",
      "Epoch 411: Train Loss = 0.5381, Val Loss = 1.1588\n",
      "Epoch 412: Train Loss = 0.5336, Val Loss = 1.0846\n",
      "Epoch 413: Train Loss = 0.5318, Val Loss = 1.1736\n",
      "Epoch 414: Train Loss = 0.5205, Val Loss = 1.0108\n",
      "Epoch 415: Train Loss = 0.5335, Val Loss = 1.1175\n",
      "Epoch 416: Train Loss = 0.5209, Val Loss = 1.1203\n",
      "Epoch 417: Train Loss = 0.5328, Val Loss = 1.0512\n",
      "Epoch 418: Train Loss = 0.5186, Val Loss = 1.0147\n",
      "Epoch 419: Train Loss = 0.5247, Val Loss = 1.0112\n",
      "Epoch 420: Train Loss = 0.5281, Val Loss = 1.0641\n",
      "Epoch 421: Train Loss = 0.5281, Val Loss = 1.2168\n",
      "Epoch 422: Train Loss = 0.5257, Val Loss = 1.0663\n",
      "Epoch 423: Train Loss = 0.5403, Val Loss = 1.1514\n",
      "Epoch 424: Train Loss = 0.5449, Val Loss = 1.1183\n",
      "Epoch 425: Train Loss = 0.5318, Val Loss = 1.1978\n",
      "Epoch 426: Train Loss = 0.5442, Val Loss = 1.2097\n",
      "Epoch 427: Train Loss = 0.5325, Val Loss = 1.0777\n",
      "Epoch 428: Train Loss = 0.5168, Val Loss = 1.0575\n",
      "Epoch 429: Train Loss = 0.5299, Val Loss = 1.1093\n",
      "Epoch 430: Train Loss = 0.5311, Val Loss = 1.0196\n",
      "Epoch 431: Train Loss = 0.5182, Val Loss = 1.0103\n",
      "Epoch 432: Train Loss = 0.5183, Val Loss = 1.0733\n",
      "Epoch 433: Train Loss = 0.5339, Val Loss = 1.1113\n",
      "Epoch 434: Train Loss = 0.5351, Val Loss = 1.0594\n",
      "Epoch 435: Train Loss = 0.5217, Val Loss = 1.0889\n",
      "Epoch 436: Train Loss = 0.5172, Val Loss = 1.1121\n",
      "Epoch 437: Train Loss = 0.5229, Val Loss = 1.2084\n",
      "Epoch 438: Train Loss = 0.5111, Val Loss = 0.9694\n",
      "Epoch 439: Train Loss = 0.5198, Val Loss = 1.1484\n",
      "Epoch 440: Train Loss = 0.5253, Val Loss = 1.0869\n",
      "Epoch 441: Train Loss = 0.5148, Val Loss = 1.0289\n",
      "Epoch 442: Train Loss = 0.5239, Val Loss = 0.9831\n",
      "Epoch 443: Train Loss = 0.5318, Val Loss = 1.1712\n",
      "Epoch 444: Train Loss = 0.5258, Val Loss = 1.0788\n",
      "Epoch 445: Train Loss = 0.5306, Val Loss = 1.0749\n",
      "Epoch 446: Train Loss = 0.5261, Val Loss = 1.1602\n",
      "Epoch 447: Train Loss = 0.5126, Val Loss = 1.0449\n",
      "Epoch 448: Train Loss = 0.5183, Val Loss = 1.1306\n",
      "Epoch 449: Train Loss = 0.5206, Val Loss = 1.0865\n",
      "Epoch 450: Train Loss = 0.5318, Val Loss = 1.0282\n",
      "Epoch 451: Train Loss = 0.5197, Val Loss = 1.0007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6b1c65ec756b>\u001b[0m in \u001b[0;36m<cell line: 238>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlm_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlm_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-6b1c65ec756b>\u001b[0m in \u001b[0;36mtrain_epoch_bert\u001b[0;34m(model, dataloader, optimizer, criterion, device, mlm_probability)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---------------------\n",
    "# 1. Special Tokens & Vocabulary\n",
    "# ---------------------\n",
    "SPECIAL_TOKENS = {\n",
    "    \"<pad>\": 0,\n",
    "    \"<bos>\": 1,\n",
    "    \"<eos>\": 2,\n",
    "    \"<mask>\": 3\n",
    "}\n",
    "\n",
    "def build_vocab(dataset):\n",
    "    \"\"\"\n",
    "    Build vocabulary from a dataset.\n",
    "    Each example is a dict with keys \"amplitude_tokens\" and \"squared_amplitude_tokens\".\n",
    "    Both lists are iterated over to add tokens to the vocabulary.\n",
    "    \"\"\"\n",
    "    vocab = dict(SPECIAL_TOKENS)\n",
    "    for example in dataset:\n",
    "        for token in example[\"amplitude_tokens\"]:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "        for token in example[\"squared_amplitude_tokens\"]:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# ---------------------\n",
    "# 2. Dataset and DataLoader\n",
    "# ---------------------\n",
    "class LanguageModelDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_len=200):\n",
    "        \"\"\"\n",
    "        data: list of dicts; each dict has keys \"amplitude_tokens\" and \"squared_amplitude_tokens\".\n",
    "              Each value is a list of token strings.\n",
    "        vocab: mapping from token to index.\n",
    "        max_len: maximum sequence length (sequence is truncated if longer).\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        # Combine tokens from both columns.\n",
    "        tokens = ([SPECIAL_TOKENS[\"<bos>\"]] +\n",
    "                  [self.vocab[token] for token in example[\"amplitude_tokens\"]] +\n",
    "                  [self.vocab[token] for token in example[\"squared_amplitude_tokens\"]] +\n",
    "                  [SPECIAL_TOKENS[\"<eos>\"]])\n",
    "        tokens = tokens[:self.max_len]\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    max_len = max(len(x) for x in batch)\n",
    "    padded = torch.full((batch_size, max_len), SPECIAL_TOKENS[\"<pad>\"], dtype=torch.long)\n",
    "    for i, seq in enumerate(batch):\n",
    "        padded[i, :len(seq)] = seq\n",
    "    return padded\n",
    "\n",
    "# ---------------------\n",
    "# 3. Positional Encoding Module\n",
    "# ---------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        Registers positional encodings with shape (max_len, 1, d_model) to be added\n",
    "        to inputs of shape (seq_len, batch, d_model).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, batch, d_model)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# ---------------------\n",
    "# 4. Masking Function for MLM\n",
    "# ---------------------\n",
    "def mask_tokens(inputs, mask_token_id, vocab_size, mlm_probability=0.15):\n",
    "    \"\"\"\n",
    "    Prepare masked tokens inputs/labels for masked language modeling.\n",
    "    inputs: Tensor of shape (batch, seq_len)\n",
    "    Returns: masked_inputs, labels (labels is -100 for tokens that are not masked)\n",
    "    \"\"\"\n",
    "    # Ensure we create new tensors on the same device as inputs.\n",
    "    labels = inputs.clone()\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability, device=inputs.device)\n",
    "    special_tokens_mask = (inputs == SPECIAL_TOKENS[\"<bos>\"]) | \\\n",
    "                          (inputs == SPECIAL_TOKENS[\"<eos>\"]) | \\\n",
    "                          (inputs == SPECIAL_TOKENS[\"<pad>\"])\n",
    "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # Only compute loss on masked tokens.\n",
    "\n",
    "    # 80% of the time, replace masked tokens with <mask>\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8, device=inputs.device)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = mask_token_id\n",
    "\n",
    "    # 10% of the time, replace masked tokens with a random token\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5, device=inputs.device)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(low=0, high=vocab_size, size=inputs.shape, device=inputs.device)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "    # The rest 10% keep the original token.\n",
    "    return inputs, labels\n",
    "\n",
    "# ---------------------\n",
    "# 5. BERT-Style Model for Masked Language Modeling\n",
    "# ---------------------\n",
    "class BertForMaskedLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=6, dropout=0.1):\n",
    "        \"\"\"\n",
    "        A BERT-style model for masked language modeling.\n",
    "        It uses a bidirectional Transformer encoder (without causal masking).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=4*d_model, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, src_padding_mask=None):\n",
    "        # src: (seq_len, batch)\n",
    "        emb = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        emb = self.pos_encoder(emb)\n",
    "        # Bidirectional encoding: no causal mask is applied.\n",
    "        out = self.transformer_encoder(emb, src_key_padding_mask=src_padding_mask)\n",
    "        logits = self.fc_out(out)  # (seq_len, batch, vocab_size)\n",
    "        return logits\n",
    "\n",
    "# ---------------------\n",
    "# 6. Helper Function for Padding Masking\n",
    "# ---------------------\n",
    "def generate_padding_mask(batch, pad_idx=SPECIAL_TOKENS[\"<pad>\"]):\n",
    "    # batch: (batch, seq_len)\n",
    "    return (batch == pad_idx)\n",
    "\n",
    "# ---------------------\n",
    "# 7. Training and Evaluation Functions for MLM\n",
    "# ---------------------\n",
    "def train_epoch_bert(model, dataloader, optimizer, criterion, device, mlm_probability=0.15):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)  # (batch, seq_len)\n",
    "        masked_inputs, labels = mask_tokens(batch.clone(), SPECIAL_TOKENS[\"<mask>\"], vocab_size, mlm_probability)\n",
    "        # Transpose for Transformer: (seq_len, batch)\n",
    "        masked_inputs = masked_inputs.transpose(0, 1)\n",
    "        labels = labels.transpose(0, 1)\n",
    "        padding_mask = generate_padding_mask(batch).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(masked_inputs, src_padding_mask=padding_mask)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_bert(model, dataloader, criterion, device, mlm_probability=0.15):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            masked_inputs, labels = mask_tokens(batch.clone(), SPECIAL_TOKENS[\"<mask>\"], vocab_size, mlm_probability)\n",
    "            masked_inputs = masked_inputs.transpose(0, 1)\n",
    "            labels = labels.transpose(0, 1)\n",
    "            padding_mask = generate_padding_mask(batch).to(device)\n",
    "            logits = model(masked_inputs, src_padding_mask=padding_mask)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), labels.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ---------------------\n",
    "# 8. Display Predictions for Masked Tokens\n",
    "# ---------------------\n",
    "def display_predictions_bert(model, dataloader, vocab, num_examples=5, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    For each example, display the original sequence with masked tokens replaced by the model's prediction.\n",
    "    \"\"\"\n",
    "    rev_vocab = {v: k for k, v in vocab.items()}\n",
    "    model.eval()\n",
    "    examples_shown = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            # For display, mask the input using mask_tokens (with mlm_probability=0.15)\n",
    "            masked_inputs, _ = mask_tokens(batch.clone(), SPECIAL_TOKENS[\"<mask>\"], len(vocab), mlm_probability=0.15)\n",
    "            for i in range(masked_inputs.size(0)):\n",
    "                input_seq = masked_inputs[i].unsqueeze(0)  # (1, seq_len)\n",
    "                input_seq_t = input_seq.transpose(0, 1)      # (seq_len, 1)\n",
    "                padding_mask = generate_padding_mask(input_seq).to(device)\n",
    "                logits = model(input_seq_t, src_padding_mask=padding_mask)\n",
    "                logits = logits.transpose(0, 1)  # (1, seq_len, vocab_size)\n",
    "                predictions = torch.argmax(logits, dim=-1).squeeze(0).cpu().tolist()\n",
    "\n",
    "                # For positions that were masked in the input, use the model's prediction; else, show the original token.\n",
    "                original = batch[i].cpu().tolist()\n",
    "                display_seq = []\n",
    "                # We iterate over the original sequence. If the token in the masked input is <mask>,\n",
    "                # we pop one token from predictions.\n",
    "                for orig, inp in zip(original, input_seq[0].cpu().tolist()):\n",
    "                    if inp == SPECIAL_TOKENS[\"<mask>\"]:\n",
    "                        # If predictions list is empty, use <unk>\n",
    "                        display_seq.append(rev_vocab.get(predictions.pop(0) if predictions else None, \"<unk>\"))\n",
    "                    else:\n",
    "                        display_seq.append(rev_vocab.get(orig, \"<unk>\"))\n",
    "                print(\"Input (masked replaced by prediction):\", \" \".join(display_seq))\n",
    "                print(\"-\" * 50)\n",
    "                examples_shown += 1\n",
    "                if examples_shown >= num_examples:\n",
    "                    return\n",
    "\n",
    "# ---------------------\n",
    "# 9. Main Training Loop and Testing\n",
    "# ---------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data from JSON files.\n",
    "    # Each file is expected to be a list of examples with keys \"amplitude_tokens\" and \"squared_amplitude_tokens\".\n",
    "    with open(\"processed_data/train.json\") as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(\"processed_data/val.json\") as f:\n",
    "        val_data = json.load(f)\n",
    "    with open(\"processed_data/test.json\") as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    # Build vocabulary from combined data.\n",
    "    combined_data = train_data + val_data + test_data\n",
    "    vocab = build_vocab(combined_data)\n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "    # Create dataset by combining tokens from both columns.\n",
    "    train_dataset = LanguageModelDataset(train_data, vocab, max_len=200)\n",
    "    val_dataset = LanguageModelDataset(val_data, vocab, max_len=200)\n",
    "    test_dataset = LanguageModelDataset(test_data, vocab, max_len=200)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BertForMaskedLM(vocab_size, d_model=128, nhead=8, num_layers=6, dropout=0.1)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Use -100 as ignore_index for loss computation.\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    num_epochs = 500\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_epoch_bert(model, train_loader, optimizer, criterion, device, mlm_probability=0.15)\n",
    "        val_loss = evaluate_bert(model, val_loader, criterion, device, mlm_probability=0.15)\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Display sample predictions (masked token predictions) from the test set.\n",
    "    print(\"\\nSample Predictions on Test Data:\")\n",
    "    display_predictions_bert(model, test_loader, vocab, num_examples=5, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-25T07:30:01.763666Z",
     "iopub.status.busy": "2025-03-25T07:30:01.763374Z",
     "iopub.status.idle": "2025-03-25T07:30:03.647298Z",
     "shell.execute_reply": "2025-03-25T07:30:03.646434Z",
     "shell.execute_reply.started": "2025-03-25T07:30:01.763643Z"
    },
    "id": "gBrBYljgEUb2",
    "outputId": "8f0a0c5f-2a7b-47a1-9092-43a14fac5753",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "Test Token-Level Accuracy on Masked Positions: 0.8125\n"
     ]
    }
   ],
   "source": [
    "print(\"working\")\n",
    "def test_accuracy_bert(model, dataloader, device, mlm_probability=0.15):\n",
    "    \"\"\"\n",
    "    Computes token-level accuracy for masked language modeling on the test set.\n",
    "    Only considers tokens that were masked (i.e. labels != -100).\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): the fraction of correctly predicted masked tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_masked = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)  # (batch, seq_len)\n",
    "            # Create masked inputs and labels using the same masking function.\n",
    "            masked_inputs, labels = mask_tokens(batch.clone(), SPECIAL_TOKENS[\"<mask>\"], vocab_size, mlm_probability)\n",
    "            # Transpose for Transformer: (seq_len, batch)\n",
    "            masked_inputs = masked_inputs.transpose(0, 1)\n",
    "            labels = labels.transpose(0, 1)\n",
    "            padding_mask = generate_padding_mask(batch).to(device)\n",
    "            logits = model(masked_inputs, src_padding_mask=padding_mask)\n",
    "            # Get predictions (seq_len, batch)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # For accuracy, we compare predictions with labels where labels != -100.\n",
    "            mask = (labels != -100)\n",
    "            total_masked += mask.sum().item()\n",
    "            total_correct += (predictions[mask] == labels[mask]).sum().item()\n",
    "    accuracy = total_correct / total_masked if total_masked > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "# Example usage:\n",
    "accuracy = test_accuracy_bert(model, test_loader, device, mlm_probability=0.15)\n",
    "print(f\"Test Token-Level Accuracy on Masked Positions: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-25T07:30:07.750776Z",
     "iopub.status.busy": "2025-03-25T07:30:07.750480Z",
     "iopub.status.idle": "2025-03-25T07:30:09.818406Z",
     "shell.execute_reply": "2025-03-25T07:30:09.817441Z",
     "shell.execute_reply.started": "2025-03-25T07:30:07.750754Z"
    },
    "id": "f4FII94cHmwT",
    "outputId": "5c63f301-55af-48c1-b105-8950e6792123",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sequence Accuracy: 0.0244\n"
     ]
    }
   ],
   "source": [
    "def sequence_accuracy_bert(model, dataloader, device, mlm_probability=0.15):\n",
    "    \"\"\"\n",
    "    Compute sequence accuracy for masked language modeling on the test set.\n",
    "    For each example, we mask tokens using the same strategy as training.\n",
    "    Then, if every token in the example that was masked is predicted correctly,\n",
    "    the sequence is considered correct.\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): fraction of sequences with all masked tokens predicted correctly.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_sequences = 0\n",
    "    correct_sequences = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)  # (batch, seq_len)\n",
    "            # Create masked inputs and corresponding labels.\n",
    "            masked_inputs, labels = mask_tokens(batch.clone(), SPECIAL_TOKENS[\"<mask>\"], vocab_size, mlm_probability)\n",
    "            # Transpose for the Transformer: (seq_len, batch)\n",
    "            masked_inputs_t = masked_inputs.transpose(0, 1)\n",
    "            labels_t = labels.transpose(0, 1)\n",
    "            padding_mask = generate_padding_mask(batch).to(device)\n",
    "            logits = model(masked_inputs_t, src_padding_mask=padding_mask)\n",
    "            predictions = torch.argmax(logits, dim=-1)  # (seq_len, batch)\n",
    "\n",
    "            # Evaluate each sequence in the batch.\n",
    "            for i in range(batch.size(0)):\n",
    "                seq_labels = labels_t[:, i]\n",
    "                seq_preds = predictions[:, i]\n",
    "                # Only consider positions that were masked (labels != -100)\n",
    "                mask_positions = (seq_labels != -100)\n",
    "                # If no tokens were masked, count the sequence as correct.\n",
    "                if mask_positions.sum().item() == 0:\n",
    "                    correct_sequences += 1\n",
    "                else:\n",
    "                    if torch.equal(seq_preds[mask_positions], seq_labels[mask_positions]):\n",
    "                        correct_sequences += 1\n",
    "                total_sequences += 1\n",
    "    return correct_sequences / total_sequences if total_sequences > 0 else 0.0\n",
    "\n",
    "acc = sequence_accuracy_bert(model, test_loader, device, mlm_probability=0.15)\n",
    "print(f\"Test Sequence Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6948107,
     "sourceId": 11139086,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
